The work is proportional to the number of elements.
For a mesh of 128000 elements both a serial and 1-thread
simulation carry out the computational work in 2.5 seconds.

For a mesh of 1024000 elements both a serial and 1-thread
simulation carry out the computational work in around 20.0 seconds.   
So, eight times more work, eight times longer.

Now comes the weird part. When I use 2  threads, so that each thread
works on 512000 elements, the amount of work per thread is
10 seconds. However the work procedure shows that it consumes
around 16.5 seconds.

When I use 4 threads, each thread works on 256,000 elements,
and consequently the work procedure should execute    
in 5 seconds. However, the work procedure actually shows
that it consumes roughly 15.6 seconds.

With 8 threads, each thread works on 128,000 elements,
and the work procedure should only take 2.5 seconds.
However, it reports to take roughly 14 seconds.

The threaded execution therefore looks like this:

Number of elements  Number of threads  Execution time
per thread 
1024000                     1              20
 512000                     2              16.5
 256000                     4              15.6
 128000                     8              14     

The weird thing is I time the interior of the work procedure.
So that should exclude any overhead associated with threading.
However, as you can see the number of threads actually affects
how much time the work procedure spends doing the work. 
The total amount of time farming out the work to the threads is
very small. The total amount of time collecting the data with
`wait` pretty much is equal to the amount of time reported by 
the work procedure. As if the overhead related to threading was
very small.

The whole thing can be exercised by
```
git clone https://github.com/PetrKryslUCSD/FinEtoolsDeforNonlinear.jl
```
followed by
```
cd FinEtoolsDeforNonlinear
export JULIA_NUM_THREADS=8
julia
```
and
```
include("threaded_test.jl")
```